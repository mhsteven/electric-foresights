{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import timezone as timezone\n",
    "import time\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "import imp\n",
    "import support\n",
    "imp.reload(support)\n",
    "from support import download_links\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location={'monterrey':{'lon':-100.316673,'lat':25.66667}}\n",
    "norm_data={'monterrey':{'mean':{'Total':1066.444949,'Rate':18.983134,\n",
    "                               'Ending':54.022061,'feels_like':295.112323,\n",
    "                               'pressure':1016.545350,'humidity':67.419076,\n",
    "                               'Day_sin':-0.000163,'Day_cos':0.000212,\n",
    "                               'Week_sin':0.001562,'Week_cos':-0.001023,\n",
    "                               'Wx':-0.070782,'Wy':1.287389}\n",
    "                        ,'std':{'Total':638.168254,'Rate':0.882050,\n",
    "                               'Ending':9.875657,'feels_like':8.362288,\n",
    "                               'pressure':5.600516,'humidity':20.231055,\n",
    "                               'Day_sin':0.707119,'Day_cos':0.707119,\n",
    "                               'Week_sin':0.707117,'Week_cos':0.707118,\n",
    "                               'Wx':1.513531,'Wy':1.864332}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geteprice():\n",
    "    try:\n",
    "        !mkdir data\n",
    "    except:\n",
    "        pass\n",
    "    chromeOptions = webdriver.ChromeOptions()\n",
    "    chromeOptions.add_argument(\"--window-size=1920,1080\")\n",
    "    chromeOptions.add_argument(\"--start-maximized\")\n",
    "    chromeOptions.add_argument(\"--headless\")\n",
    "    driver=webdriver.Chrome(options=chromeOptions) #\n",
    "    driver.get('https://www.cenace.gob.mx/Paginas/SIM/Reportes/H_PreciosEnergiaSisMEM.aspx?N=26&opc=divCssPreEnergia&site=Precios%20de%20la%20energ%C3%ADa/Precios%20de%20Nodos%20Distribuidos/MDA/Diarios&tipoArch=C&tipoUni=SIN&tipo=Diarios&nombrenodop=Precios%20de%20Nodos%20Distribuidos')\n",
    "    path1='//*[@id=\"ctl00_ContentPlaceHolder1_treePrincipal\"]/ul/li[1]/ul/li[1]/div/span[2]'\n",
    "    path2='//*[@id=\"ctl00_ContentPlaceHolder1_treePrincipal\"]/ul/li[1]/ul/li[2]/div/span[2]'\n",
    "    path3='//*[@id=\"products\"]/tbody/tr[2]/td[2]/table/tbody/tr[8]/td[2]'\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, path2))\n",
    "        )\n",
    "    except:\n",
    "        print('Time over for electricity price.')\n",
    "    check1=driver.find_element(By.XPATH, path1)\n",
    "    check2=driver.find_element(By.XPATH, path2)\n",
    "    check1.click()\n",
    "    check2.click()\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, path3))\n",
    "    )\n",
    "    except:\n",
    "        print('Time over for electricity price table contents.')\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    table=soup.find('table',id='products').find('tbody').find_all('tr',{\"class\":\"orders\"})\n",
    "    links_total=[]\n",
    "    csv_re=\".csv\"\n",
    "    SIN_re='_SIN_'\n",
    "    for order in table:\n",
    "        try:\n",
    "            for tr in order.find('tbody').find_all('tr'):\n",
    "                tds=tr.find_all('td')\n",
    "                try:\n",
    "                    date=tds[0].text.strip().replace('/','-')\n",
    "                    link=tds[1].find('a')['href'].replace('../../..','https://www.cenace.gob.mx')\n",
    "                    if re.search(csv_re,link)!=None and re.search(SIN_re,link)!=None:\n",
    "                        links_total.append((date,link))\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    p=Pool(4)\n",
    "    p.starmap(download_links,links_total)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    driver.quit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to determin the starting row\n",
    "def starting_row(fname):\n",
    "    with open(fname) as f:\n",
    "        for i,l in enumerate(f):\n",
    "            if 'Hora' in l:\n",
    "                return i+1\n",
    "\n",
    "#Timestamp convertion\n",
    "def TS_convert(hour):\n",
    "    global day,month,year\n",
    "    hour=int(hour)-1\n",
    "    if hour<24:\n",
    "        time=datetime.datetime(year=year,month=month,day=day,hour=hour)\n",
    "        return time\n",
    "    else:\n",
    "        return None\n",
    "day,month,year=0,0,0\n",
    "def generate_pricepd():\n",
    "    files=glob.glob('./data/*')\n",
    "    rule='(20\\d\\d)-(\\d\\d)-(\\d\\d)'\n",
    "    #Initialize full dataset\n",
    "    dataset_full=pd.DataFrame()\n",
    "\n",
    "    #initialize global variables\n",
    "    global day,month,year\n",
    "\n",
    "    for file in files:\n",
    "        year,month,day=re.findall(rule,file)[0]\n",
    "        day=int(day)\n",
    "        month=int(month)\n",
    "        year=int(year)\n",
    "        temp_df=pd.read_csv(file,skip_blank_lines=False,skiprows=starting_row(file),names=['Hour','Region','Total','Energy','Lost','Congestion'])\n",
    "        temp_df['Timestamp']=temp_df['Hour'].apply(TS_convert)\n",
    "        temp_df['Date']=datetime.datetime(year=year,month=month,day=day).date()\n",
    "        temp_df=temp_df[temp_df['Timestamp'].notnull()]\n",
    "        dataset_full=pd.concat([dataset_full,temp_df])\n",
    "        dataset_full=dataset_full.sort_values(by=['Region','Timestamp'])\n",
    "    return dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Crude oil price\n",
    "def get_crude(end):\n",
    "    today = (datetime.date.today()-datetime.timedelta(days=1))\n",
    "    start = today-datetime.timedelta(days=30)\n",
    "    url='https://cn.investing.com/commodities/crude-oil-historical-data'\n",
    "    chromeOptions = webdriver.ChromeOptions()\n",
    "    chromeOptions.add_argument(\"--window-size=1920,1080\")\n",
    "    chromeOptions.add_argument(\"--start-maximized\")\n",
    "    chromeOptions.add_argument(\"--headless\")\n",
    "\n",
    "    \n",
    "    driver=webdriver.Chrome(options=chromeOptions)#\n",
    "    driver.get(url)\n",
    "    year=start.year\n",
    "    month=start.month\n",
    "    day=start.day\n",
    "    sdate=str(year)+'/'+str(month)+'/'+str(day)\n",
    "    WebDriverWait(driver, 50).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"widgetFieldDateRange\"]'))).click()\n",
    "    element = driver.find_element_by_id(\"startDate\")\n",
    "    element.clear()\n",
    "    element.send_keys(sdate)\n",
    "    WebDriverWait(driver, 50).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"applyBtn\"]'))).click()\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"curr_table\"]/tbody/tr[1]/td[1]'))\n",
    "        )\n",
    "    except:\n",
    "        print('No Table loaded')\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    table=soup.find_all('tbody')[1].find_all('tr')\n",
    "    crude_oil_data=[]\n",
    "\n",
    "    for tr in table:\n",
    "        temp=tr.find_all('td')\n",
    "        date=datetime.datetime.fromtimestamp(float(temp[0].attrs['data-real-value'])).date()\n",
    "        price=float(temp[1].attrs['data-real-value'])\n",
    "        crude_oil_data.append([date,price])\n",
    "    headings=['Date','Ending']\n",
    "    Crude_df=pd.DataFrame(data=crude_oil_data,columns=headings)\n",
    "    #noticed some days doesn't have a price. Taking the prior day price\n",
    "    mindate=Crude_df['Date'].min()\n",
    "    \n",
    "    index=mindate\n",
    "    while index <= end:\n",
    "        if len(Crude_df[Crude_df['Date']==index])==0:\n",
    "            prior=index-datetime.timedelta(days=1)\n",
    "            price=float(Crude_df[Crude_df['Date']==prior]['Ending'])\n",
    "            temp=pd.DataFrame(data=[[index,price]],columns=headings)\n",
    "            Crude_df=pd.concat([Crude_df,temp])\n",
    "        index+=datetime.timedelta(days=1)\n",
    "    Crude_df=Crude_df.sort_values('Date').reset_index()[['Date','Ending']]\n",
    "    driver.quit()\n",
    "    return Crude_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exchange(end):\n",
    "    url='https://www.currency-converter.org.uk/currency-rates/historical/table/USD-MXN.html'\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    date_re='(\\d+)/(\\d+)/(\\d\\d\\d\\d)'\n",
    "    table=soup.find_all('table')[0].find_all('tr')[1:]\n",
    "    exchange_data=[]\n",
    "    for tr in table:\n",
    "        temp=tr.find_all('td')\n",
    "        try:\n",
    "            date=re.findall(date_re,temp[1].text)[0]\n",
    "        except:\n",
    "            print(temp)\n",
    "        day=int(date[0])\n",
    "        month=int(date[1])\n",
    "        year=int(date[2])\n",
    "        rate=float(temp[3].text.split(' ')[0])\n",
    "        exchange_data.append([datetime.datetime(year,month,day).date(),rate])\n",
    "    headings=['Date','Rate']\n",
    "    exchange_pd=pd.DataFrame(data=exchange_data,columns=headings)\n",
    "    maxdate=exchange_pd['Date'].max()\n",
    "    if end>maxdate:\n",
    "        while maxdate+datetime.timedelta(days=1)<=end:\n",
    "            pri_rate=exchange_pd[exchange_pd['Date']==maxdate]['Rate'].tolist()[0]\n",
    "            temp_pd=pd.DataFrame(data=[[maxdate+datetime.timedelta(days=1),pri_rate]],columns=headings)\n",
    "            exchange_pd=pd.concat([exchange_pd,temp_pd]).reset_index(drop = True)\n",
    "            maxdate+=datetime.timedelta(days=1)\n",
    "    return exchange_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_lookup={'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,\n",
    "              'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "def st_conversion(row):\n",
    "    Mth,day=row['Date'].split(' ')\n",
    "    Mth=months_lookup[Mth]\n",
    "    day=int(day)\n",
    "    row['Timestamp']=datetime.datetime(row['Year'],Mth,day)\n",
    "    return row\n",
    "\n",
    "def get_holiday():\n",
    "    headings=['Day', 'Date', 'Holiday Name', 'Type', 'Comments','Year']\n",
    "    holiday_data=pd.DataFrame()\n",
    "    today=datetime.date.today()\n",
    "    cur_year=today.year\n",
    "    prior_year=cur_year-1\n",
    "    year_list=[prior_year,cur_year]\n",
    "    \n",
    "    for year in year_list:\n",
    "        url='https://www.officeholidays.com/countries/mexico/'+str(year)\n",
    "        html_content = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        H_table_obj = soup.find(\"table\", attrs={\"class\": \"country-table\"})\n",
    "        table=H_table_obj.tbody.find_all(\"tr\")\n",
    "\n",
    "        # Get all the rows of table\n",
    "        table_data = []\n",
    "        for tr in table: # find holiday_data.iloc[1]['Date'].split(' ')all tr's from table's tbody\n",
    "            row=[]\n",
    "            for td in tr.find_all('td'): \n",
    "                row.append(td.text.replace('\\n', '').strip())\n",
    "            row.append(year)\n",
    "            table_data.append(row)\n",
    "        \n",
    "        temp_pd=pd.DataFrame(data=table_data,columns=headings)\n",
    "        holiday_data=pd.concat([holiday_data,temp_pd])\n",
    "    \n",
    "    holiday_data['Timestamp']=None\n",
    "    holiday_data=holiday_data.apply(st_conversion,axis=1)\n",
    "    return holiday_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def daily_weather_scrapper(day_num,city):\n",
    "    city=city.lower()\n",
    "    latitude=str(location[city]['lat'])\n",
    "    longitude=str(location[city]['lon'])\n",
    "    units='standard' #return temperature in kelvin\n",
    "    appid = \"b5100260400cfb368667e820bb4b58f0\"\n",
    "    day = requests.get(\"https://api.openweathermap.org/data/2.5/onecall/\"\\\n",
    "                        +\"timemachine?lat=\"+latitude\\\n",
    "                        +\"&lon=\"+longitude\\\n",
    "                        +\"&units=\"+units\\\n",
    "                        +\"&dt=\"+str(day_num)\\\n",
    "                        +\"&appid=\"+appid)\n",
    "    day_scrapped = pd.DataFrame(day.json()['hourly'])\n",
    "    day_scrapped['dt_iso'] = day_scrapped['dt'].map(lambda x:datetime.datetime.fromtimestamp(x,timezone.utc).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    day_scrapped['city_name']=city\n",
    "    day_scrapped = day_scrapped[['dt','dt_iso','city_name','temp','feels_like'\\\n",
    "                                 ,'pressure','humidity'\\\n",
    "                                 ,'wind_speed','wind_deg']]\n",
    "    return day_scrapped\n",
    "\n",
    "\n",
    "\n",
    "def daily_weather_forecast(day_num,city):\n",
    "    city=city.lower()\n",
    "    latitude=str(location[city]['lat'])\n",
    "    longitude=str(location[city]['lon'])\n",
    "    units='standard' #return temperature in kelvin\n",
    "    appid = \"b5100260400cfb368667e820bb4b58f0\"\n",
    "    day = requests.get(\"https://api.openweathermap.org/data/2.5/onecall?\"\\\n",
    "                        +\"lat=\"+latitude\\\n",
    "                        +\"&lon=\"+longitude\\\n",
    "                        +\"&exclude=minutely,daily&appid=b5100260400cfb368667e820bb4b58f0\")\n",
    "    day_scrapped = pd.DataFrame(day.json()['hourly'])\n",
    "    day_scrapped['dt_iso'] = day_scrapped['dt'].map(lambda x:datetime.datetime.fromtimestamp(x,timezone.utc).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    day_scrapped['city_name']=city\n",
    "    day_scrapped = day_scrapped[['dt','dt_iso','city_name','temp','feels_like'\\\n",
    "                                 ,'pressure','humidity'\\\n",
    "                                 ,'wind_speed','wind_deg']]\n",
    "    return day_scrapped\n",
    "\n",
    "\n",
    "\n",
    "def get_weather():\n",
    "    ahora = int(time.time()) #current unix day\n",
    "    hora_previa = ahora - (ahora % (60*60*24));#-18000;\n",
    "    # datetime.utcfromtimestamp(hora_previa).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    days1 = hora_previa - (1*(60*60*24))\n",
    "    days2 = hora_previa - (2*(60*60*24))\n",
    "    days3 = hora_previa - (3*(60*60*24))\n",
    "    days4 = hora_previa - (4*(60*60*24))\n",
    "    days5 = hora_previa - (5*(60*60*24))\n",
    "    days_list = [days5, days4, days3, days2, days1,hora_previa]\n",
    "    \n",
    "    historical_data=pd.read_csv('historical_weather.csv')\n",
    "    historical_data=historical_data[historical_data['dt']<hora_previa]\n",
    "    \n",
    "    for city in location.keys():\n",
    "        data_list= [daily_weather_scrapper(i,city) for i in days_list]\n",
    "        f_data=daily_weather_forecast(hora_previa,city)\n",
    "        historical_data['city_name']=historical_data['city_name'].map(lambda x:x.lower())\n",
    "        temp=historical_data[historical_data['city_name']==city.lower()]\n",
    "        maxdate=temp['dt'].max()\n",
    "        n=0\n",
    "        error_msg=''\n",
    "        for n,item in enumerate(days_list):\n",
    "            if item>maxdate:\n",
    "                break;\n",
    "        else:\n",
    "            error_msg='updated'\n",
    "        if error_msg!='updated':\n",
    "            data_list=data_list[n:]\n",
    "            previous5_days = pd.concat(data_list).reset_index(drop = True)\n",
    "            historical_data = pd.concat([historical_data,previous5_days]).reset_index(drop = True)\n",
    "        historical_data = pd.concat([historical_data,f_data]).reset_index(drop = True)\n",
    "        historical_data.to_csv('historical_weather.csv')\n",
    "    return historical_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataset,timecol,hour=1,day=0):\n",
    "    dataset=dataset.sort_values(timecol)\n",
    "    start_date=dataset[timecol].min()\n",
    "    end_date=dataset[timecol].max()\n",
    "    current = start_date\n",
    "    #impute missing value\n",
    "#     print('missing:')\n",
    "    while current<end_date:\n",
    "        if len(dataset[dataset[timecol]==current])==0:\n",
    "            prior=current-datetime.timedelta(hours=hour,days=day)\n",
    "            temp=dataset[dataset[timecol]==prior].copy(deep=True)\n",
    "#             print(current)\n",
    "            temp[timecol]=current\n",
    "            dataset=pd.concat([dataset,temp])\n",
    "        \n",
    "        current+=datetime.timedelta(hours=hour,days=day)\n",
    "    dataset=dataset.reset_index(drop = True)\n",
    "    dataset=dataset.sort_values(timecol)\n",
    "    #remove duplicates\n",
    "#     print('duplicates')\n",
    "    current = start_date\n",
    "    dataset['Filter']=True\n",
    "    while current<end_date:\n",
    "        if len(dataset[dataset[timecol]==current])>1:\n",
    "#             print(current)\n",
    "            temp=dataset[dataset[timecol]==current].iloc[1:]\n",
    "            dataset.at[temp.index,'Filter']=False\n",
    "\n",
    "        current+=datetime.timedelta(hours=hour,days=day)\n",
    "    dataset=dataset[dataset.Filter==True].reset_index(drop = True)\n",
    "    dataset=dataset.drop('Filter',axis=1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#Consolidation of dataset\n",
    "def consol_data(city,lmp_data,ex_data,crude_pd,weather_pd):\n",
    "    lmp_data['Date']=pd.to_datetime(lmp_data['Date'])\n",
    "    lmp_data['Timestamp']=pd.to_datetime(lmp_data['Timestamp'])\n",
    "    lmp_hourly_data=lmp_data[lmp_data['Region']==city.upper()]\n",
    "    lmp_hourly_data=lmp_hourly_data.reset_index()\n",
    "    lmp_hourly_data=validation(lmp_hourly_data,'Timestamp')\n",
    "    \n",
    "    ex_data['Date']=pd.to_datetime(ex_data['Date'])-datetime.timedelta(days=1)\n",
    "    ex_data=validation(ex_data,'Date',0,1)\n",
    "    combined_pd=lmp_hourly_data.join(ex_data.set_index('Date'),on='Date',how='inner')\n",
    "    combined_pd=combined_pd.reset_index()[['Date','Timestamp','Energy','Lost','Congestion','Rate']]\n",
    "    \n",
    "    crude_pd['Date']=pd.to_datetime(crude_pd['Date'])-datetime.timedelta(days=1)\n",
    "    crude_pd=validation(crude_pd,'Date',0,1)\n",
    "    combined_pd=combined_pd.join(crude_pd.set_index('Date'),on='Date',how='inner')\n",
    "    \n",
    "    weather_pd=weather_pd[weather_pd['city_name']==city]\n",
    "    weather_pd.dt_iso=weather_pd.dt_iso.apply(lambda x:x.replace(' +0000 UTC',''))\n",
    "    weather_pd.dt_iso=pd.to_datetime(weather_pd.dt_iso)\n",
    "    weather_pd=validation(weather_pd,'dt_iso')\n",
    "    weather_pd=weather_pd.rename(columns={'dt_iso':'Timestamp'})\n",
    "    combined_pd=combined_pd.join(weather_pd.set_index('Timestamp'),on='Timestamp',how='inner')\n",
    "    combined_pd['Total']=combined_pd['Energy']+combined_pd['Lost']+combined_pd['Congestion']\n",
    "    output_pd=combined_pd.copy(deep=True)\n",
    "    #feature engineering\n",
    "    features=['Total', 'Rate', 'Ending','feels_like','pressure','humidity','wind_speed','wind_deg']\n",
    "    combined_pd['Timestamp']=combined_pd['Timestamp'].map(datetime.datetime.timestamp)\n",
    "    data_features=combined_pd[features]\n",
    "    \n",
    "    #Time features\n",
    "    day=60*60*24\n",
    "    week=7*day\n",
    "    year=day*365.25\n",
    "\n",
    "    data_features['Day_sin']=np.sin(combined_pd['Timestamp']*(2*np.pi/day))\n",
    "    data_features['Day_cos']=np.cos(combined_pd['Timestamp']*(2*np.pi/day))\n",
    "    data_features['Week_sin']=np.sin(combined_pd['Timestamp']*(2*np.pi/week))\n",
    "    data_features['Week_cos']=np.cos(combined_pd['Timestamp']*(2*np.pi/week))\n",
    "\n",
    "    # Feature Engineering Wind Velocity Vectors\n",
    "    ws = data_features.pop('wind_speed')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = data_features.pop('wind_deg')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    data_features['Wx'] = ws*np.cos(wd_rad)\n",
    "    data_features['Wy'] = ws*np.sin(wd_rad)\n",
    "    \n",
    "    # Normalization\n",
    "    for feature in norm_data[city]['mean'].keys():\n",
    "        data_features[feature]=data_features[feature].map(lambda x:(x-norm_data[city]['mean'][feature])/norm_data[city]['std'][feature])\n",
    "    \n",
    "    output_pd=output_pd[['Timestamp','Total', 'Rate', 'Ending','feels_like']]\n",
    "    \n",
    "    return data_features[-168*2:],output_pd[-168*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataget():\n",
    "    today = datetime.date.today()-datetime.timedelta(days=1)\n",
    "    start = today-datetime.timedelta(days=13)\n",
    "    #scrap electricity data\n",
    "    geteprice()\n",
    "    elec_price=generate_pricepd()\n",
    "    maxdate=elec_price['Date'].max()+datetime.timedelta(days=1)\n",
    "    maxdate=maxdate\n",
    "    #scrap crude oil price\n",
    "    crude_price=get_crude(maxdate)\n",
    "    \n",
    "    #scrap exchange rate\n",
    "    exchange_rate=get_exchange(maxdate)\n",
    "    \n",
    "#     #scrap holiday data\n",
    "#     holiday_info=get_holiday()\n",
    "    \n",
    "    #scrap weather data\n",
    "    weather_info=get_weather()\n",
    "    \n",
    "    return elec_price,crude_price,exchange_rate,weather_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b96341fd142c>:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features['Day_sin']=np.sin(combined_pd['Timestamp']*(2*np.pi/day))\n",
      "<ipython-input-9-b96341fd142c>:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features['Day_cos']=np.cos(combined_pd['Timestamp']*(2*np.pi/day))\n",
      "<ipython-input-9-b96341fd142c>:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features['Week_sin']=np.sin(combined_pd['Timestamp']*(2*np.pi/week))\n",
      "<ipython-input-9-b96341fd142c>:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features['Week_cos']=np.cos(combined_pd['Timestamp']*(2*np.pi/week))\n",
      "<ipython-input-9-b96341fd142c>:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features['Wx'] = ws*np.cos(wd_rad)\n",
      "<ipython-input-9-b96341fd142c>:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features['Wy'] = ws*np.sin(wd_rad)\n",
      "<ipython-input-9-b96341fd142c>:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_features[feature]=data_features[feature].map(lambda x:(x-norm_data[city]['mean'][feature])/norm_data[city]['std'][feature])\n"
     ]
    }
   ],
   "source": [
    "elec_price,crude_price,exchange_rate,weather_info=dataget()\n",
    "\n",
    "# #consolidation\n",
    "dataset,output_set=consol_data('monterrey',elec_price,exchange_rate,crude_price,weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_STEPS = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa2a1b38070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "  def __init__(self, units, out_steps):\n",
    "    super().__init__()\n",
    "    self.out_steps = out_steps\n",
    "    self.units = units\n",
    "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(12)\n",
    "feedback_model = FeedBack(units=680, out_steps=OUT_STEPS)\n",
    "def warmup(self, inputs):\n",
    "  # inputs.shape => (batch, time, features)\n",
    "  # x.shape => (batch, lstm_units)\n",
    "  x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "  # predictions.shape => (batch, features)\n",
    "  prediction = self.dense(x)\n",
    "  return prediction, state\n",
    "\n",
    "FeedBack.warmup = warmup\n",
    "def call(self, inputs, training=None):\n",
    "  # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "  predictions = []\n",
    "  # Initialize the lstm state\n",
    "  prediction, state = self.warmup(inputs)\n",
    "\n",
    "  # Insert the first prediction\n",
    "  predictions.append(prediction)\n",
    "\n",
    "  # Run the rest of the prediction steps\n",
    "  for n in range(1, self.out_steps):\n",
    "    # Use the last prediction as input.\n",
    "    x = prediction\n",
    "    # Execute one lstm step.\n",
    "    x, state = self.lstm_cell(x, states=state,\n",
    "                              training=training)\n",
    "    # Convert the lstm output to a prediction.\n",
    "    prediction = self.dense(x)\n",
    "    # Add the prediction to the output\n",
    "    predictions.append(prediction)\n",
    "\n",
    "  # predictions.shape => (time, batch, features)\n",
    "  predictions = tf.stack(predictions)\n",
    "  # predictions.shape => (batch, time, features)\n",
    "  predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "  return predictions\n",
    "\n",
    "FeedBack.call = call\n",
    "feedback_model.load_weights('./Models/HLMP_AutoRegress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=tf.expand_dims(tf.convert_to_tensor(dataset),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer feed_back is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=feedback_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=tf.squeeze(predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi=output_set['Timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[maxi+datetime.timedelta(hours=x+1) for x in range(168)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘prediction’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "output_list=['Total','Rate','Ending','feels_like']\n",
    "try:\n",
    "    !mkdir prediction\n",
    "except:\n",
    "    pass \n",
    "for n,feature in enumerate(output_list):\n",
    "    data=[]\n",
    "    subdata=result[:,n].numpy()*norm_data['monterrey']['std'][feature]+norm_data['monterrey']['mean'][feature]\n",
    "    for i,date in enumerate(dates):\n",
    "        data.append([date,subdata[i]])\n",
    "    temp=pd.DataFrame(data=data,columns=['Timestamp',feature])\n",
    "    title='./prediction/monterrey_'+feature+'_P.csv'\n",
    "    temp.to_csv(title)\n",
    "    title='./prediction/monterrey_'+feature+'_A.csv'\n",
    "    temp=output_set[['Timestamp',feature]].to_csv(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
